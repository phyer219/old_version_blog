#+TITLE: Susskind's Statistical Mechanics
#+DATE: <2021-03-11>
#+CATEGORIES: 专业笔记
#+TAGS: physics, Statistical Mechanics, Susskind
#+HTML: <!-- toc -->
#+HTML: <!-- more -->

* Information
- 官方介绍: https://theoreticalminimum.com/courses/statistical-mechanics/2013/spring

- 视频列表地址: https://www.youtube.com/watch?v=D1RzvXDXyqA&list=PL_IkS0viawhr3HcKH607rXbVqy28W_gB7

- 参考 Note: https://www.lapasserelle.com/statistical_mechanics/index.html

- 把主要内容整理一下.

* Lecture 1: Entropy and conservation of information

** Conservation of Information

一个六面 die, 六个面有不同的颜色 blue, red, green, purple, yellow, orange, 如图
[[file:2021-03-11-专业笔记-SusskindsStatisticalMechanics/die.png]]
如果它以某种规律运动, 比如下图中红色的箭头
[[file:2021-03-11-专业笔记-SusskindsStatisticalMechanics/ring1.png]]
那么无论何时, 发现它处于某一面的概率都是相等的
\begin{align}
P(i) = \frac{1}{6}
\end{align}
$i$ 代表第 $i$ 面朝上的事件.
如果它按蓝色箭头, 结果是一样的, $P(i) = \frac{1}{6}$ . 又比如, 它按下面的方式运
动
[[file:2021-03-11-专业笔记-SusskindsStatisticalMechanics/ring2.png]]
它会有两种状态. 就是说, 它可以处于上面的循环中, 记这个状态为 $+1$ , 那么在这个状
态中, 某个面向上的概率是 $P(+1) = \frac{1}{3}$ , 它永远不会跳到下面的态.

它也可以处下面的态, 记为 $+1$ , 同样的 $P(+1) = \frac{1}{3}$ .

上面的例子都是一些好的运动规律, good laws, 因为它的 infromation conserved. 也就是说, 不论过了多
久, 我们知道系统的信息一样的.

但是也有一些不好的例子, bad laws, 比如下图
[[file:2021-03-11-专业笔记-SusskindsStatisticalMechanics/ring_bad.png]]
它在演化的过程中会 lose information , 这个 information 指初态的 infromation. 比
如一开始处于 orange, 过了一会, 它一定会到 red. 它是不可逆的.

这种 bad laws 在实际的运动中可以对应比如
\begin{align}
  \frac{\mathrm{d}^2x_n}{\mathrm{d}t^2} = -\gamma \frac{\mathrm{d}x_n}{\mathrm{d}t}
\end{align}
它最终的结果是 erery particles come to rest.

conservation of information , minus first law.

** Entropy

$N$ is TOTAL number of STATES. 其中 $M < N$ 个态上的占据概率相等, 为
$P=\frac{1}{M}$ , 其它态上的占据概率是 $0$ . 那么定义 entropy
\begin{align}
  S = \log M
\end{align}
Entropy: Measures approximately the number of states which have non-zero
probability. The bigger it, the less you know.

In space of states(phase space) of real mechanics, the volume of phase space
will stay the same.(Liouville's theorem) i.e. the same number of states.

** How to define entropy for a complex probability distribution?

\begin{align}
  S = - \sum_iP(i) \log P(i)
\end{align}
它可以回到简单的情况. 对于 $P(i) = \frac{1}{M}$ , others zero
\begin{align}
  S = -\sum_i \frac{1}{M}\log \frac{1}{M} = \log M
\end{align}
特例, $M = 1$ , $S = 0$ , Complete knowledge, Entropy zero.

** Definition of Entropy in Phase Space

[[file:2021-03-11-专业笔记-SusskindsStatisticalMechanics/entropy_phase_space.png]]
如果像上图一样, 概率是均匀的, 那么
\begin{align}
  S = \log V_{PH}
\end{align}
$V_{PH}$ is volume of phase space.

如果概率不均匀, 那么也可以扩展成
\begin{align}
 S = - \int \mathrm{d}p\mathrm{d}x P(p, x) \log P(p, x)
\end{align}
它满足
\begin{align}
  \int P(p, x)\mathrm{d}p \mathrm{d}x = 1
\end{align}

* Lecture 2: Units & Temperature

** Units declaration

$c = \hbar = G = k_{\mathrm{B}} = 1$ . Temperature change is a human construct that
was invented for convenience.
\begin{align}
  T = k_{\mathrm{B}}t_{\mathrm{K}}
\end{align}
$t_{\mathrm{K}}$ is temperature in Kelvin.

** What temperature is?

The average energy in state $i$ which average energy is $E$ is
\begin{align}
  \sum_iP(i, E) E_i = \langle E\rangle
\end{align}
the entropy is
\begin{align}
  S(E) = -\sum_i P(i, E)\log P(i, E)
\end{align}
由于
\begin{align}
  \sum_iP(i, E) = 1
\end{align}
所以当比较大的 $E(i)$ 概率变大的化, 整个概率分布看起来会更平. 如下图
[[file:2021-03-11-专业笔记-SusskindsStatisticalMechanics/P_i_E.png]]
蓝线到橙线到时红线它们的平均能量 $E$ 是变大的, 同时概率分布变得理平, 意味着
entropy 也增大.

Consider: How much do you have to change the average energy in order to change
the entropy by $1$ bit( $\log 2$ )? We define temperature like this
\begin{align}
\Delta E = \frac{\partial E}{\partial S} \Delta S
\end{align}

\begin{align}
\mathrm{d}E = T \mathrm{d}S
\end{align}

** It is temperature

下面证明它就是温度, 也就是说两个系统达到热平衡时, 上面定义的温度是相同的.

考虑两个系统 $A, B$ , 初始时它们的温度分别为 $T_A, T_B$ 并且 $T_B > T_A$ . First
law says
\begin{align}
  \mathrm{d}E_A + \mathrm{d}E_B =0
\end{align}
second law says
\begin{align}
  \mathrm{d}S_A + \mathrm{d}S_B > 0
\end{align}
combine them and use our definition of temperature we get
\begin{align}
 \mathrm{d}S_B =& \frac{\mathrm{d}E_B}{T_B} = - \frac{T_A}{T_B}\mathrm{d}S_A \\
          \Downarrow &\\
 0 < & \mathrm{d}S_A + \mathrm{d}S_B = \left(1 - \frac{T_A}{T_B}\right) \mathrm{d}S_A
\end{align}
becasue $T_B > T_A$ , so
\begin{align}
 \mathrm{d}S_A > & 0\\
          \Downarrow &\\
 T_A \mathrm{d} S_A > & 0 \\
          \Downarrow &\\
 \mathrm{d}E_A > & 0
\end{align}
so energy flowed from B to A.

也就是说能量会从温度高的地方向温度低的地方流动, 也就是说只有温度相同的, 才没有能
量流动, 也就是说达到了热平衡.

* Lecture 3: Maximizing entropy

Zeroth law: there is a notion of temperature which way energy flows. Ultimately
equilibrium.

** A Trick (Canonical Ensemble)

Imagine that the system in question is one of a very large number of identical
systems which are connected together by little pipes that allow heat to flow
back and forth. One of them is the system we're studying. The rest of them
simply provide the heat bath.
[[file:2021-03-11-专业笔记-SusskindsStatisticalMechanics/ensemble.png]]
We're going to let them get a large number $N$ of them. Large enough that we can
think of the heat bath is very big.

Each one of the systems is in a state. Call the number of systems in the
$i\mathrm{th}$ state $n_i = (n_1, n_2, n_3, \cdots)$
Constrains
\begin{align}
\sum_in_i =& N \\
\sum_in_i E_i =& N E
\end{align}

** Maximizing entropy

What is the probability that a given one of these systems is in the
$i\mathrm{th}$ state? It is
\begin{align}
P(i) = \frac{n_i}{N}
\end{align}
use this to rewrite the constrains
\begin{align}
  \sum_iP(i) =& 1 \\
  \sum_iP(i) E_i =& E
\end{align}
Given the occupation numbers, how many ways of redistributing
the states?(most of occupation number is zero) That is
\begin{align}
  \mathrm{Number}\quad \mathrm{of} \quad\mathrm{arrangements}
    =\frac{N!}{\Pi _in_i!}
\end{align}
use Stirling's Approximation $N! \sim N^Ne^{- N}$ as $N\to\infty$
\begin{align}
\log\frac{N!}{\Pi _in_i!} \approx& \log \frac{N^N}{\sum_in_i^{n_i}}
  = N\log N - \sum_in_i\log n_i \\
  =& N\log N - \sum_iN P(i)\log [NP(i)] \\
  =& - N \sum_i P(i) \log P(i)
\end{align}
It's entropy. If we want to find out the occupation numbers which maximize the
number of ways that you can rearrange the system keep the occupation numbers
fixed. It simply corresponds to maximizing the entropy.

The most probable distribution of occupation numbers corresponds to
probabilities which maximize the entropy.

* Lecture 4: The Boltzmann distribution

** Partition function

Use Lagrange Multiplier to maximize the entropy
\begin{align}
  F'(P) = - \sum_iP_i \log P_i  - \alpha \left[\sum_i P_i - 1 \right] - \beta\left[\sum_iE_iP_i - E\right]
\end{align}
let $\frac{\partial F'}{\partial P_i} = 0$, we get
\begin{align}
  P_i = e^{-(1 + \alpha)} e^{- \beta E_i}
\end{align}
call $e^{-(1 + \alpha)} = Z$ partition function, then
\begin{align}
  P_i = \frac{1}{Z} e^{- \beta E_i}
\end{align}
这个分布是在给定平均能量下最有可能的分布.

so partition function is
\begin{align}
  Z = \sum_i e^{- \beta E_i}
\end{align}
Tuning $\beta$ , tuning average energy. 限制温度相当于限制平均能量.

$E_{i}$ 是系统给定的.

关于 $E, \alpha, \beta$ 之间的关系. $\alpha, \beta$ 是拉氏乘子, 它对应两个约束, 一个是概率归一,
一个是能量给定. 结果就是, 由概率归一和能量给定, 就可以得出 $\alpha, \beta$ . 而概率归一
是永远知道的, 所以剩下的三个量 $E, \alpha, \beta$ 之中给定任何一个, 就可以结合概率归一
得出任剩下的两个, 也就是 $E(\alpha), E(\beta), \alpha(E), \alpha(\beta), \beta(E), \beta(\alpha)$ . 而 $\beta$
是一个我们非常关心的物理量, 下面将会说明它是温度的倒数, 所以常用的关系就是
$E(\beta)$ 和 $\alpha(\beta)$ , 也就是 $Z(\beta)$ , 因为 $Z$ 和 $\alpha$ 只是做了一个变量替换而已.
得到了 $Z(\beta)$ 就得到了系统所有的其它热力学量.

** Average Energy $E$

如果我们知道了 $Z(\beta)$ , 当然可以得到 $E$ , 因为 $E(Z) = E(Z(\beta)) = E(\beta)$ . 也
就是说利用关系 $E(Z)$ , $Z(\beta)$ 得到 $E(\beta)$
\begin{align}
  E = \sum_i P_iE_i = \sum_i \frac{1}{Z(\beta)} e^{-\beta E_i}E_i
        = - \frac{1}{Z(\beta)}\frac{\partial Z(\beta)}{\partial\beta}
        = - \frac{\partial}{\partial\beta}\log Z(\beta)
\end{align}

** Entropy

\begin{align}
S = -\sum_i P_i\log P_i
\end{align}
而
\begin{align}
 P_i = \frac{1}{Z(\beta)}e^{-\beta E_i}
\end{align}
so
\begin{align}
S =& -\sum_i P_i\left[ -\beta E_i - \log Z \right] \\
  =& \beta \sum_i P_iE_i + \sum_iP_i\log Z \\
  =& \beta E(\beta) +\log Z(\beta)
\end{align}

** Temperature

\begin{align}
\mathrm{d}S = \beta \mathrm{d}E + E \mathrm{d}\beta +\frac{\partial \log Z}{\partial \beta}\mathrm{d}\beta
    = \beta \mathrm{d}E + E \mathrm{d}\beta -E \mathrm{d}\beta = \beta \mathrm{d}E
\end{align}
so
\begin{align}
 T = \frac{\mathrm{d}E}{\mathrm{d}S} = \frac{1}{\beta}
\end{align}

** Summary

\begin{align}
P_i =& \frac{1}{Z} e^{-\beta E_i} \\
Z =& \sum_i e^{- \beta E_i} \\
E =& - \frac{\partial}{\partial\beta} \log Z \\
T =& \frac{1}{\beta} \\
S =& \beta E + \log Z
\end{align}

** Example: the Ideal Gas

在体积 $V$ 内的 $N$ 个粒子. States: collection $x_1, \cdots, x_{3N}, p_1, p_{3N}$
. $6N$ 维的相空间中的每个点对应一个 state. 每个 state 的能量为 $\sum_{n=1}^{3N}
\frac{p^2_n}{2m}$ .
\begin{align}
 Z =& \frac{1}{N!} \int \mathrm{d}^{3N}x \int \mathrm{d}^{3N}p\cdot e^{- \beta\sum_{n=1}^{3N}
          \frac{p^2_n}{2m}} \\
   =& \frac{V^N}{N!} \left(\frac{2\pi m}{\beta} \right)^{3N/2}
\end{align}
(因子 $\frac{1}{N!}$ 是因为假设粒子不可分辨, 但对结果无影响)
\begin{align}
\log Z = - \frac{3N}{2} \log\beta + \mathrm{const.}
\end{align}
\begin{align}
E = - \frac{\partial}{\partial\beta}\log Z = \frac{3}{2}NT
\end{align}

* Lecture 5: Pressure of an Ideal Gas & Fluctuations

** Helmholtz free energy

\begin{align}
S =& \frac{1}{T} E + \log Z
\end{align}
First define a very useful variable Helmholtz free energy
\begin{align}
  A = E - TS = -T \log Z
\end{align}

** Maxwell' Relation

它是一个纯数学的关系. 假设 $E, S$ 和 $T, V$ 是两组独立的变量, 那么
\begin{align}
\left.\frac{\partial E}{\partial V}\right|_S = \left.\frac{\partial E}{\partial V}\right|_T
    - \left.\frac{\partial E}{\partial S}\right|_V \left.\frac{\partial S}{\partial V}\right|_T
\end{align}

从物理的角度理解, 考虑一个系统体积可变, 如下图, 加一个活塞
[[file:2021-03-11-专业笔记-SusskindsStatisticalMechanics/piston.png]]
那此时, 我们不变量由原来只有 $T$ , 变成了两个变量 $T, V$ . 那以 $T, V$ 为坐标,
画出一条 entropy 的等高线, 在这个线上 entropy 是不变的.
[[file:2021-03-11-专业笔记-SusskindsStatisticalMechanics/adiabatic.png]]
如果我们从这条线上的一点移到另一点, 有
\begin{align}
  \Delta S  = \left.\frac{\partial S}{\partial V}\right|_T \Delta V
              +\left.\frac{\partial S}{\partial T}\right|_V \Delta T = 0
\end{align}
同时, 能量的改变为
\begin{align}
  \Delta E  = \left.\frac{\partial E}{\partial V}\right|_T \Delta V
              +\left.\frac{\partial E}{\partial T}\right|_V \Delta T
\end{align}

* Interlude

** Stirling's Approximation

** Lagrange Multiplier


* Reference

-
